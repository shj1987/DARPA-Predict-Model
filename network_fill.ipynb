{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from load import load_data\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from math import ceil\n",
    "\n",
    "with open('/data/leidos_extracted/2021CP5/cp5_eval_nodes.txt', 'r') as f:\n",
    "    nodeList = set(sorted(set(f.read().split('\\n')[:-2])))\n",
    "\n",
    "# with open('/data/leidos_extracted/2021CP5/prev_user_to_6_29.pkl', 'rb') as infile:\n",
    "#     user_tuple = pickle.load(infile)\n",
    "#     user_tuple = user_tuple[user_tuple.nodeUserID.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['nodeID', 'nodeUserID', 'parentID', 'rootID', 'actionType', 'nodeTime', 'platform', 'rootUserID', 'parentUserID', 'informationID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_data = load_data('/data/leidos_extracted/2021CP5/cp5_twitter_2020-03-30_2020-06-29.json', ignore_first_line=False, verbose=False)[columns]\n",
    "t1_data = load_data('/data/leidos_extracted/2021CP5/cp5_twitter_2020-06-30_2020-07-06.json', ignore_first_line=False, verbose=False)[columns]\n",
    "y0_data = load_data('/data/leidos_extracted/2021CP5/cp5_youtube_to_7_06.json', ignore_first_line=False, verbose=False)[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_history = pd.concat([\n",
    "    t0_data, t1_data\n",
    "], ignore_index=True).sort_values(['nodeTime', 'nodeID']).reset_index(drop=True)\n",
    "youtube_history = pd.concat([\n",
    "    y0_data\n",
    "], ignore_index=True).sort_values(['nodeTime', 'nodeID']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_history = twitter_history[~twitter_history.nodeUserID.isnull()]\n",
    "twitter_history.loc[twitter_history.parentUserID.isnull(), 'parentUserID'] = '?'\n",
    "twitter_history.loc[twitter_history.rootUserID.isnull(), 'rootUserID'] = '?'\n",
    "youtube_history = youtube_history[~youtube_history.nodeUserID.isnull()]\n",
    "youtube_history.loc[youtube_history.parentUserID.isnull(), 'parentUserID'] = '?'\n",
    "youtube_history.loc[youtube_history.rootUserID.isnull(), 'rootUserID'] = '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_history = twitter_history[(twitter_history.nodeTime < pd.to_datetime('2020-06-09'))]\n",
    "youtube_history = youtube_history[(youtube_history.nodeTime < pd.to_datetime('2020-06-09'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append = 'to_7_06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/leidos_extracted/2021CP5/twitter_prob_{}.json'.format(append), 'r') as f:\n",
    "    twitter_prob = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/leidos_extracted/2021CP5/youtube_prob_{}.json'.format(append), 'r') as f:\n",
    "    youtube_prob = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range(pd.to_datetime('2020-06-09'), pd.to_datetime('2020-07-06'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkFilling(history, pred_path, prob):\n",
    "    with open(pred_path, 'r') as f:\n",
    "        d = json.loads(f.read())\n",
    "    data = {k: pd.read_json(v, orient='columns').reindex(idx, fill_value=0) for k, v in d.items()}\n",
    "    \n",
    "#     for infoID in nodeList:\n",
    "    def inner(infoID):\n",
    "        content = data[infoID]\n",
    "        totalEvent = sum(content.EventCount)\n",
    "        if totalEvent == 0:\n",
    "            return pd.DataFrame(columns=columns)\n",
    "        \n",
    "        duplicate = history[history.informationID == infoID]\n",
    "        if len(duplicate) == 0:\n",
    "            duplicate = history[history.informationID == 'controversies/china/funding'].copy() # china/naval in youtube has no history\n",
    "            duplicate.loc[:, 'informationID'] = 'controversies/china/naval'\n",
    "            active_level = pd.DataFrame(prob['prob']['controversies/china/funding']).T.reset_index().replace(0, np.inf) # china/naval in youtube has no history\n",
    "        else:\n",
    "            active_level = pd.DataFrame(prob['prob'][infoID]).T.reset_index().replace(0, np.inf)\n",
    "            \n",
    "        print(infoID, len(duplicate), int(ceil(totalEvent / len(duplicate))))\n",
    "        def mod(df, cnt):\n",
    "            tmp = df.copy()\n",
    "            tmp.nodeID = tmp.nodeID.apply(lambda x: x + '_' + str(cnt))\n",
    "            tmp.parentID = tmp.parentID.apply(lambda x: x if x == '?' else x + '_' + str(cnt))\n",
    "            tmp.rootID = tmp.rootID.apply(lambda x: x if x == '?' else x + '_' + str(cnt))\n",
    "            tmp.nodeUserID = tmp.nodeUserID.apply(lambda x: x + '_' + str(cnt))\n",
    "            tmp.parentUserID = tmp.parentUserID.apply(lambda x: x if x == '?' else x + '_' + str(cnt))\n",
    "            tmp.rootUserID = tmp.rootUserID.apply(lambda x: x if x == '?' else x + '_' + str(cnt))\n",
    "            return tmp\n",
    "        copies = [mod(duplicate, i) for i in range(1, int(ceil(totalEvent / len(duplicate))))]\n",
    "        duplicate = pd.concat(list(reversed(copies)) + [duplicate], axis=0)\n",
    "        display(totalEvent)\n",
    "        duplicate = duplicate.iloc[-totalEvent:]\n",
    "\n",
    "        counter = 0\n",
    "        maxEvent = prob['max'][infoID]\n",
    "        sortCols = {}\n",
    "        for index, (timestamp, (volume, _, newUser)) in enumerate(content.iterrows()):\n",
    "            if volume < maxEvent / 4:\n",
    "                sortCols[timestamp] = 'c0'\n",
    "            elif volume < maxEvent * 2 / 4:\n",
    "                sortCols[timestamp] = 'c1'\n",
    "            elif volume < maxEvent * 3 / 4:\n",
    "                sortCols[timestamp] = 'c2'\n",
    "            else:\n",
    "                sortCols[timestamp] = 'c3'\n",
    "                \n",
    "            print (index, infoID, timestamp, volume, sortCols[timestamp])\n",
    "            if volume == 0: continue\n",
    "\n",
    "            # NodeTime Shift\n",
    "            duplicate.nodeTime.iloc[counter: counter + volume] = list(pd.Series(pd.date_range(timestamp + pd.Timedelta(seconds=1), timestamp + pd.Timedelta(hours=23, minutes=59, seconds=59), periods=volume)).astype('datetime64[s]'))\n",
    "            # NodeID Shift\n",
    "            duplicate.nodeID.iloc[counter: counter + volume] = duplicate.nodeID.iloc[counter: counter + volume].apply(lambda x: x + '_')\n",
    "            duplicate.parentID.iloc[counter: counter + volume] = duplicate.parentID.iloc[counter: counter + volume].apply(lambda x: x if x == '?' else x + '_')\n",
    "            duplicate.rootID.iloc[counter: counter + volume] = duplicate.rootID.iloc[counter: counter + volume].apply(lambda x: x if x == '?' else x + '_')\n",
    "\n",
    "            counter += volume\n",
    "            \n",
    "        involved_users = []\n",
    "        grouped_data = duplicate[['nodeID', 'nodeUserID', 'nodeTime']].set_index('nodeTime').groupby([pd.Grouper(freq='1D'), 'nodeUserID'])\\\n",
    "            .nodeID.nunique().to_frame().reset_index().sort_values(['nodeTime', 'nodeID'], ascending=[True, True])\\\n",
    "            .drop_duplicates('nodeUserID')\n",
    "        grouped_data['_nodeUserID'] = grouped_data.nodeUserID.apply(lambda x: x[:22])\n",
    "        active_level.columns = ['_nodeUserID', 'c0', 'c1', 'c2', 'c3']\n",
    "        grouped_data = grouped_data.merge(active_level, how='left', on='_nodeUserID').set_index('nodeTime').groupby([pd.Grouper(freq='1D')])\n",
    "\n",
    "        for name, group in grouped_data:\n",
    "            group = group.sort_values(['nodeID', sortCols[name]])\n",
    "            involved_users.extend(list(group.head(content.loc[name].NewUserCount).nodeUserID.values))\n",
    "        involved_users = set(involved_users)\n",
    "        duplicate.nodeUserID = duplicate.nodeUserID.apply(lambda x: x + '_' if x in involved_users else x)\n",
    "        duplicate.parentUserID = duplicate.parentUserID.apply(lambda x: x if x == '?' else (x + '_' if x in involved_users else x))\n",
    "        duplicate.rootUserID = duplicate.rootUserID.apply(lambda x: x if x == '?' else (x + '_' if x in involved_users else x))\n",
    "        \n",
    "        return duplicate\n",
    "    final = Parallel(n_jobs=8, verbose=50)(delayed(inner)(infoID) for infoID in data.keys())\n",
    "    final = pd.concat(final)\n",
    "    return data, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(data, df):\n",
    "    for infoID in data.keys():\n",
    "        if data[infoID].EventCount.sum() == 0:\n",
    "            print('Pass', infoID)\n",
    "            continue\n",
    "        print(np.all(data[infoID].EventCount == df[df.informationID == infoID].nodeTime.value_counts().resample('D').sum().reindex(data['empty'].index, fill_value=0)), infoID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'UIUC_ML_LASSO_2_CORR_LEIDOS': '/home/michael/local_eval/'\n",
    "}\n",
    "output_path = '/home/dachun/cp5/local0706/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(models['UIUC_ML_LASSO_2_CORR_LEIDOS'] + 'twitter_UIUC_ML_LASSO_2_CORR_LEIDOS.json', 'r') as f:\n",
    "    d = json.loads(f.read())\n",
    "data = {k: pd.read_json(v, orient='columns') for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name, model_path in models.items():\n",
    "    print(model_name)\n",
    "    ydata, ydf = networkFilling(youtube_history, model_path + 'youtube_{}.json'.format(model_name), youtube_prob)\n",
    "    check(ydata, ydf)\n",
    "    tdata, tdf = networkFilling(twitter_history, model_path + 'twitter_{}.json'.format(model_name), twitter_prob)\n",
    "    check(tdata, tdf)\n",
    "    final = pd.concat([tdf, ydf], axis=0).sort_values(by='nodeTime').reset_index(drop=True)\n",
    "    final.loc[~final.informationID.isin(nodeList), 'informationID'] = 'other'\n",
    "    final.nodeTime = final.nodeTime.apply(lambda x: x.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "    identifier = {\"team\": \"uiuc\", \"model_identifier\": model_name, \"simulation_period\": \"june9-july6\"}\n",
    "\n",
    "    with open(output_path + model_name + '.json', 'w') as f:\n",
    "        f.write(json.dumps(identifier) + '\\n')\n",
    "        for row in tqdm(final.iterrows(), total=final.shape[0]):\n",
    "            f.write(row[1].to_json() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
