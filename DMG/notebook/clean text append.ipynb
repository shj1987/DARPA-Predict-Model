{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "url2results = dict()\n",
    "with open('news_text_en_phrased.json') as fin:\n",
    "    for line in fin:\n",
    "        js = json.loads(line)\n",
    "        url2results[js['url']] = js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6863it [00:00, 8166.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339876\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "url2news = dict()\n",
    "news = []\n",
    "with gzip.open('append_news_raw.json.gz', 'rt') as fin:\n",
    "    for line in tqdm(fin):\n",
    "        obj = json.loads(line)\n",
    "        nid = len(news)\n",
    "        news.append(obj)\n",
    "        url2news[news[-1]['url']] = len(news) - 1\n",
    "        \n",
    "import json\n",
    "from collections import defaultdict\n",
    "events = []\n",
    "for line in gzip.open('cp5-cpec.exogenous.gdelt.events.v1.json.gz', 'rt'):\n",
    "    events.append(json.loads(line.strip()))\n",
    "print(len(events))\n",
    "url2code = defaultdict(list)\n",
    "for idx, js in enumerate(events):\n",
    "    url2code[js['sourceurl']].append(idx)\n",
    "    \n",
    "for u in set(url2code) & set(url2news):\n",
    "    if len(url2code[u]) > 0:\n",
    "        news[url2news[u]]['events'] = url2code[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6863/6863 [00:00<00:00, 73476.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract dates\n",
    "for i in news:\n",
    "    ti = None\n",
    "    if 'article_extracted_date' in i['extension']:\n",
    "        ti = i['extension']['article_extracted_date']\n",
    "    if not ti:\n",
    "        ti = i['extension']['twitter_reference_datetime']\n",
    "        if ti:\n",
    "            ti = ti.split('T')[0]\n",
    "    if not ti:\n",
    "        ti = i['extension']['gdelt_reference_datetime']\n",
    "        if ti:\n",
    "            ti = ti.split('T')[0]\n",
    "    i['date'] = ti\n",
    "narratives = ['benefits/connections/afghanistan', 'benefits/covid', 'benefits/development/energy', 'benefits/development/maritime', 'benefits/development/roads', 'benefits/jobs', 'controversies/china/border', 'controversies/china/debt', 'controversies/china/exploitation', 'controversies/china/funding', 'controversies/china/naval', 'controversies/china/uighur', 'controversies/pakistan/army', 'controversies/pakistan/bajwa', 'controversies/pakistan/baloch', 'controversies/pakistan/students', 'leadership/bajwa', 'leadership/khan', 'leadership/sharif', 'opposition/kashmir', 'opposition/propaganda']\n",
    "print(len(narratives))\n",
    "\n",
    "import fasttext\n",
    "fasttext.FastText.eprint = print\n",
    "lid_model = fasttext.load_model('lid.176.ftz')\n",
    "for x in tqdm(news):\n",
    "    try:\n",
    "        text = x['title'].replace('\\n', ' ')\n",
    "        lang = lid_model.predict(text)[0][0][-2:]\n",
    "        assert len(lang) == 2\n",
    "    except:\n",
    "        lang = 'unkown'\n",
    "    x['lang'] = lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6863/6863 [00:00<00:00, 31381.75it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('news_text_raw_append.json', 'w') as fout:\n",
    "    for i in tqdm(news):\n",
    "        if 'title' in i:\n",
    "            fout.write(json.dumps({'title':i['title'],\n",
    "                        'article':i['text'],\n",
    "                        'date':i['date'],\n",
    "                        'url':i['url'],\n",
    "                        'lang':i['lang']}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6863/6863 [00:02<00:00, 2970.25it/s]\n",
      "100%|██████████| 2166/2166 [00:00<00:00, 1315312.36it/s]\n",
      "100%|██████████| 1919/1919 [00:00<00:00, 9117.77it/s]\n",
      "100%|██████████| 1919/1919 [01:43<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate: 0.6551070960221478\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('words')\n",
    "# tc = nltk.classify.textcat.TextCat()\n",
    "import fasttext\n",
    "fasttext.FastText.eprint = print\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "\n",
    "ascii = set(string.printable)\n",
    "\n",
    "from collections import defaultdict\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "clean_text = []\n",
    "spliter = [' | ', ' - ', ' — ', ' -- ', ' – ', ' » ']\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "printable = set(string.printable)\n",
    "\n",
    "def clean0(i, newline=True): # make ' and \" right\n",
    "    i = re.sub(r'“', '\"', i)\n",
    "    i = re.sub(r'”', '\"', i)\n",
    "    i = re.sub(r'‘', \"'\", i)\n",
    "    i = re.sub(r'’', \"'\", i)\n",
    "    i = ''.join(filter(lambda x: x in printable, i))\n",
    "    return i.strip(' ')\n",
    "def clean1(text, title_raw): # remove html codes in text\n",
    "    tmp = []\n",
    "    ban_words = ['(http)', '(\\.com)', '( = )', '(==)', '(&&)', '(\\|\\|)', '(© Copyright )', 'Copyright ©', 'function\\s.+\\(.*\\)\\s*\\{'] # '( \\| )', \n",
    "    for i in text.split('\\n'):\n",
    "        if len(i.split()) >= 5:\n",
    "            if not re.search('|'.join(ban_words), i) and i != title_raw:\n",
    "                tmp.append(clean0(i))\n",
    "    return tmp\n",
    "def clean3(doc, spliter='\\n'): # segement into sentences\n",
    "    if type(doc) == list:\n",
    "        doc = '\\n'.join(doc)\n",
    "    doc = nlp(doc[:10000])\n",
    "    tmp = []\n",
    "    for i in doc.sents:\n",
    "        tmp.append(' '.join(map(str, i)))\n",
    "    return spliter.join(tmp)\n",
    "def clean4(texts): # remove duplicated sentences in article\n",
    "    url_set = set()\n",
    "    title_set = set()\n",
    "    tmp = []\n",
    "    for js in tqdm(texts):\n",
    "        u = js['title']\n",
    "        if js['url']:\n",
    "            u = js['url']\n",
    "        t = js['title']\n",
    "        if u in url_set or t in title_set:\n",
    "            continue\n",
    "        url_set.add(u)\n",
    "        title_set.add(t)\n",
    "        tmp.append(js)\n",
    "    return tmp    \n",
    "def clean2(texts): # remove duplicated sentences in article\n",
    "    count = defaultdict(int)\n",
    "    for js in tqdm(texts):\n",
    "        for sent in js['article']:\n",
    "            _ = re.sub('\\d+', '@num@', sent).lower()\n",
    "            count[_] += 1\n",
    "    for js in tqdm(texts):\n",
    "        doc = []\n",
    "        for sent in js['article']:\n",
    "            _ = re.sub('\\d+', '@num@', sent).lower()\n",
    "            if count[_] <= 1:\n",
    "                doc.append(sent)\n",
    "        title = clean3(js['title'], ' ')\n",
    "        article = clean3(doc)\n",
    "        js['title'] = title\n",
    "        js['article'] = article\n",
    "\n",
    "err = 0\n",
    "for idx, i in enumerate(tqdm(news)):\n",
    "    if 'title' in i and i['title'] and i['lang'] == 'en':\n",
    "        text = i['title'].replace('\\n', ' ')\n",
    "        for s in spliter:\n",
    "            if text.find(s) != -1:\n",
    "                tmp = []\n",
    "                for j in text.split(s):\n",
    "                    tmp.append(j)\n",
    "                tmp = sorted(tmp, key=lambda x:-len(x))\n",
    "                text = tmp[0]\n",
    "        if len(text.split()) >= 5 and len(text) >= 5:\n",
    "            clean_text.append({'title':clean0(text[:10000], newline=False),\n",
    "                               'article':clean1(i['text'][:10000], i['title'][:10000]),\n",
    "                               'date':i['date'],\n",
    "                               'url':i['url'],\n",
    "                               'lang':i['lang']\n",
    "                              })\n",
    "    else:\n",
    "        err += 1\n",
    "clean_text = clean4(clean_text)\n",
    "clean2(clean_text)\n",
    "\n",
    "print('error rate:', err / len(news))\n",
    "import json\n",
    "# with open(folder + f'news_text_en_append.json', 'w', encoding='utf-8') as fout:\n",
    "#     for js in clean_text:\n",
    "#         fout.write(json.dumps(js) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#phrases 155576\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "from flashtext import KeywordProcessor\n",
    "keyword_processor = KeywordProcessor()\n",
    "phrase2score = dict()\n",
    "with open('AutoPhrase.txt') as fin:\n",
    "    for line in fin:\n",
    "        score, phrase = line.strip().split('\\t')\n",
    "        keyword_processor.add_keyword(phrase, phrase.replace(' ', '_'))\n",
    "        phrase = phrase.replace(' ', '_')\n",
    "        phrases.append(phrase)\n",
    "        phrase2score[phrase] = float(score)\n",
    "print('#phrases', len(phrases))\n",
    "import re\n",
    "\n",
    "for js in clean_text:\n",
    "    text = (js['title'].strip()).replace('\\n', ' ').replace('\\r', ' ')\n",
    "    if len(text) > 0:\n",
    "        js['phrased_title'] = keyword_processor.replace_keywords(text)\n",
    "    else:\n",
    "        js['phrased_title'] = ''\n",
    "    text = (js['article'].strip()).replace('\\n', ' ').replace('\\r', ' ')\n",
    "    if len(text) > 0:\n",
    "        js['phrased_article'] = keyword_processor.replace_keywords(text)\n",
    "    else:\n",
    "        js['phrased_article'] = ''\n",
    "\n",
    "import json\n",
    "with open(folder + f'news_text_en_phrased_append.json', 'w', encoding='utf-8') as fout:\n",
    "    for js in clean_text:\n",
    "        fout.write(json.dumps(js) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
