{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T19:59:05.090511Z",
     "start_time": "2021-01-15T19:59:01.563647Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import AlbertTokenizer, AlbertModel, AlbertConfig\n",
    "from transformers import AdamW\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning import loggers as pl_logger\n",
    "from sklearn.metrics import f1_score\n",
    "from pytorch_lightning.metrics import F1, Precision, Recall\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "def encoding_text(d):\n",
    "    inputs = tokenizer.encode(d, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=256)\n",
    "    return inputs.squeeze()\n",
    "\n",
    "\n",
    "class BertClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cls = transformers.BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased')\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.crossentropy = torch.nn.CrossEntropyLoss()\n",
    "        self.f1 = F1()\n",
    "        self.p = Precision()\n",
    "        self.r = Recall()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.cls(x).logits)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, label = batch\n",
    "        pred = self.forward(x)\n",
    "        loss = self.crossentropy(pred, label)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, label = batch\n",
    "        logits = self.forward(x)\n",
    "        return label, logits[:,1]\n",
    "    \n",
    "    def evaluate_outs(self, outs):\n",
    "        labels = torch.cat([label for label, pred in outs])\n",
    "        preds = torch.cat([pred for label, pred in outs])\n",
    "        f1 = self.f1(preds, labels)\n",
    "        p = self.p(preds, labels)\n",
    "        r = self.r(preds, labels)\n",
    "        return f1, p, r\n",
    "    \n",
    "    def validation_epoch_end(self, outs):\n",
    "        f1, p, r = self.evaluate_outs(outs)\n",
    "        print('f1', f1)\n",
    "        print('p', p)\n",
    "        print('r', r)\n",
    "        self.log(\"val_f1\", f1)\n",
    "        self.log(\"val_p\", p)\n",
    "        self.log(\"val_r\", r)\n",
    "        return f1\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "    \n",
    "    def test_epoch_end(self, outs):\n",
    "        f1, p, r = self.evaluate_outs(outs)\n",
    "        self.log(\"test_f1\", f1)\n",
    "        self.log(\"test_p\", p)\n",
    "        self.log(\"test_r\", r)\n",
    "        return f1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=lr)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T19:59:05.327323Z",
     "start_time": "2021-01-15T19:59:05.092547Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "import json\n",
    "class binary_dataset(Dataset):\n",
    "    def __init__(self, fn, maximum_training=-1):\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        with open(fn) as fin:\n",
    "            for line in fin:\n",
    "                js = json.loads(line)\n",
    "                self.Y.append(js['label'])\n",
    "                self.X.append(js['text'])\n",
    "                if maximum_training != -1 and len(self.X) > maximum_training:\n",
    "                    break\n",
    "    def __getitem__(self, idx):\n",
    "        return encoding_text(self.X[idx]), int(self.Y[idx])\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T19:59:32.289821Z",
     "start_time": "2021-01-15T19:59:21.585289Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "lr = 1e-4\n",
    "warmup_steps = 50\n",
    "batch_size = 80\n",
    "maximum_training = 1000000\n",
    "epochs = 8\n",
    "\n",
    "dataset = binary_dataset('tweet_text_for_relevance.json', maximum_training)\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = int(0.1 * len(dataset))\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "num_training_steps = len(train_set) // batch_size * epochs\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=128, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=128)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_f1',\n",
    "    dirpath='./',\n",
    "    filename='relevance_model_best_f1',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "trainer = pl.Trainer(max_epochs=epochs, gpus=[0], precision=16, default_root_dir='checkpoints', callbacks=[checkpoint_callback])\n",
    "\n",
    "model = BertClassifier()\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "model = BertClassifier.load_from_checkpoint('./relevance_model_best_f1.ckpt')\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=128)\n",
    "trainer.test(model, test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finetuned on tweet\n",
    "validation:\n",
    "[{'test_f1': 0.8882,\n",
    "  'test_p': 0.9177,\n",
    "  'test_r': 0.8605}]\n",
    "\n",
    "TEST RESULTS (tweet)\n",
    " [{'test_f1': 0.8901,\n",
    "  'test_p': 0.9180,\n",
    "  'test_r': 0.8637}]\n",
    " \n",
    " TEST RESULTS (news, direct apply)\n",
    "[{'test_f1': 0.2075,\n",
    "  'test_p': 1.0,\n",
    "  'test_r': 0.1158}]\n",
    "  \n",
    "  \n",
    "news, finetuned, validation\n",
    "{'test_f1': tensor(0.8485, device='cuda:0'),\n",
    " 'test_p': tensor(0.9333, device='cuda:0'),\n",
    " 'test_r': tensor(0.7778, device='cuda:0')}\n",
    " \n",
    "news, finetuned, testing (29, 145 in total)\n",
    "{'test_f1': tensor(0.8718, device='cuda:0'),\n",
    " 'test_p': tensor(0.8947, device='cuda:0'),\n",
    " 'test_r': tensor(0.8500, device='cuda:0')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly apply tweet model for news\n",
    "model = BertClassifier.load_from_checkpoint('./relevance_model_best_f1.ckpt')\n",
    "news_set = binary_dataset('news_text_for_relevance.json')\n",
    "news_loader = DataLoader(news_set, batch_size=72, num_workers=128)\n",
    "trainer.test(model, test_dataloaders=news_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "lr = 1e-4\n",
    "warmup_steps = 3\n",
    "batch_size = 12\n",
    "batch_size_eval = 256\n",
    "epochs = 100\n",
    "\n",
    "news_set = binary_dataset('news_text_for_relevance.json')\n",
    "print(len(news_set))\n",
    "\n",
    "val_size = int(0.2 * len(news_set))\n",
    "test_size = int(0.2 * len(news_set))\n",
    "train_size = len(news_set) - val_size - test_size\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(news_set, [train_size, val_size, test_size])\n",
    "print('#test', len(test_set))\n",
    "num_training_steps = len(news_set) // batch_size * epochs\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=128, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size_eval, num_workers=128)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_f1',\n",
    "    dirpath='./',\n",
    "    filename='relevance_model_news_best_f1',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "trainer = pl.Trainer(max_epochs=epochs, gpus=[0], precision=16, default_root_dir='checkpoints_news', callbacks=[checkpoint_callback])\n",
    "\n",
    "model = BertClassifier.load_from_checkpoint('./relevance_model_best_f1.ckpt')\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "model = BertClassifier.load_from_checkpoint('./relevance_model_news_best_f1.ckpt')\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size_eval, num_workers=128)\n",
    "trainer.test(model, test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = BertClassifier.load_from_checkpoint('./relevance_model_news_best_f1.ckpt')\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size_eval, num_workers=128)\n",
    "trainer.test(model, test_dataloaders=val_loader)\n",
    "trainer.test(model, test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate relevance score for all news\n",
    "model = BertClassifier.load_from_checkpoint('./relevance_model_news_best_f1.ckpt')\n",
    "class inference_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.X = []\n",
    "        with open('../news_text_raw.json') as fin:\n",
    "            for line in fin:\n",
    "                i = json.loads(line)\n",
    "                text = (i['title'] if i['title'] else ' ') + ' ' + (i['article'] if i['article'] else ' ')\n",
    "                self.X.append(text)\n",
    "    def __getitem__(self, idx):\n",
    "        return encoding_text(self.X[idx])\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "inference_set = inference_dataset()\n",
    "inference_loader = DataLoader(inference_set, batch_size=256, num_workers=128, shuffle=False, drop_last=False)\n",
    "relevance_scores = []\n",
    "relevance_predictions = []\n",
    "with torch.no_grad():\n",
    "    model.eval().to('cuda')\n",
    "    for batch in tqdm(inference_loader):\n",
    "        tmp = model(batch.to('cuda'))[:,1].detach().cpu().numpy()\n",
    "        relevance_scores.extend(tmp)\n",
    "        relevance_predictions.extend(tmp > 0.5)\n",
    "all_data = []\n",
    "with open('../news_text_raw.json') as fin:\n",
    "    for line in fin:\n",
    "        i = json.loads(line)\n",
    "        all_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2gt = dict()\n",
    "with open('news_text_for_relevance.json') as fin:\n",
    "    for line in fin:\n",
    "        js = json.loads(line)\n",
    "        url2gt[js['url']] = js['label']\n",
    "assert len(all_data) == len(relevance_scores)\n",
    "with open('news_relevance.json', 'w') as fout:\n",
    "    for js, score, pred in zip(all_data, relevance_scores, relevance_predictions):\n",
    "        if js['url'] in url2gt:\n",
    "            js['relevance_prediction'] = url2gt[js['url']]\n",
    "            js['relevance_score'] = 1.0 if js['relevance_prediction'] else 0.0\n",
    "        else:\n",
    "            js['relevance_score'] = float(score)\n",
    "            js['relevance_prediction'] = bool(pred)\n",
    "        fout.write(json.dumps(js) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for js in all_data:\n",
    "    if js['relevance_prediction'] and js['lang'] == 'en':\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
