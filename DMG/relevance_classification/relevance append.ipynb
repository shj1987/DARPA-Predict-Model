{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 26/26 [00:36<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import AlbertTokenizer, AlbertModel, AlbertConfig\n",
    "from transformers import AdamW\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning import loggers as pl_logger\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from pytorch_lightning.metrics import F1, Precision, Recall\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "def encoding_text(d):\n",
    "    inputs = tokenizer.encode(d, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=256)\n",
    "    return inputs.squeeze()\n",
    "\n",
    "\n",
    "class BertClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cls = transformers.BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased')\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.crossentropy = torch.nn.CrossEntropyLoss()\n",
    "        self.f1 = F1()\n",
    "        self.p = Precision()\n",
    "        self.r = Recall()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.cls(x).logits)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, label = batch\n",
    "        pred = self.forward(x)\n",
    "        loss = self.crossentropy(pred, label)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, label = batch\n",
    "        logits = self.forward(x)\n",
    "        return label, logits[:,1]\n",
    "    \n",
    "    def evaluate_outs(self, outs):\n",
    "        labels = torch.cat([label for label, pred in outs])\n",
    "        preds = torch.cat([pred for label, pred in outs])\n",
    "        f1 = self.f1(preds, labels)\n",
    "        p = self.p(preds, labels)\n",
    "        r = self.r(preds, labels)\n",
    "        return f1, p, r\n",
    "    \n",
    "    def validation_epoch_end(self, outs):\n",
    "        f1, p, r = self.evaluate_outs(outs)\n",
    "        print('f1', f1)\n",
    "        print('p', p)\n",
    "        print('r', r)\n",
    "        self.log(\"val_f1\", f1)\n",
    "        self.log(\"val_p\", p)\n",
    "        self.log(\"val_r\", r)\n",
    "        return f1\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "    \n",
    "    def test_epoch_end(self, outs):\n",
    "        f1, p, r = self.evaluate_outs(outs)\n",
    "        self.log(\"test_f1\", f1)\n",
    "        self.log(\"test_p\", p)\n",
    "        self.log(\"test_r\", r)\n",
    "        return f1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=lr)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "import json\n",
    "class binary_dataset(Dataset):\n",
    "    def __init__(self, fn, maximum_training=-1):\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        with open(fn) as fin:\n",
    "            for line in fin:\n",
    "                js = json.loads(line)\n",
    "                self.Y.append(js['label'])\n",
    "                self.X.append(js['text'])\n",
    "                if maximum_training != -1 and len(self.X) > maximum_training:\n",
    "                    break\n",
    "    def __getitem__(self, idx):\n",
    "        return encoding_text(self.X[idx]), int(self.Y[idx])\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "best_threshold = 4.1355447e-06\n",
    "# generate relevance score for all news\n",
    "model = BertClassifier.load_from_checkpoint('./relevance_model_best_f1.ckpt')\n",
    "class inference_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.X = []\n",
    "        with open('../news_text_raw_append.json') as fin:\n",
    "            for line in fin:\n",
    "                i = json.loads(line)\n",
    "                text = (i['title'] if i['title'] else ' ') + ' ' + (i['article'] if i['article'] else ' ')\n",
    "                self.X.append(text)\n",
    "    def __getitem__(self, idx):\n",
    "        return encoding_text(self.X[idx])\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "inference_set = inference_dataset()\n",
    "inference_loader = DataLoader(inference_set, batch_size=256, num_workers=128, shuffle=False, drop_last=False)\n",
    "relevance_scores = []\n",
    "relevance_predictions = []\n",
    "with torch.no_grad():\n",
    "    model.eval().to('cuda')\n",
    "    for batch in tqdm(inference_loader):\n",
    "        tmp = model(batch.to('cuda'))[:,1].detach().cpu().numpy()\n",
    "        relevance_scores.extend(tmp)\n",
    "        relevance_predictions.extend(tmp >= best_threshold)\n",
    "all_data = []\n",
    "with open('../news_text_raw_append.json') as fin:\n",
    "    for line in fin:\n",
    "        i = json.loads(line)\n",
    "        all_data.append(i)\n",
    "url2gt = dict()\n",
    "with open('tweet_news_text_for_relevance.json') as fin:\n",
    "    for line in fin:\n",
    "        js = json.loads(line)\n",
    "        url2gt[js['url']] = js['label']\n",
    "with open('manual_news_text_for_relevance.json') as fin:\n",
    "    for line in fin:\n",
    "        js = json.loads(line)\n",
    "        url2gt[js['url']] = js['label']\n",
    "assert len(all_data) == len(relevance_scores)\n",
    "with open('url2relevance_append.json', 'w') as fout:\n",
    "    for js, score, pred in zip(all_data, relevance_scores, relevance_predictions):\n",
    "        if js['url'] in url2gt:\n",
    "            js['relevance_prediction'] = url2gt[js['url']]\n",
    "            js['relevance_score'] = 1.0 if js['relevance_prediction'] else 0.0\n",
    "        else:\n",
    "            js['relevance_score'] = float(score)\n",
    "            js['relevance_prediction'] = bool(pred)\n",
    "        fout.write(json.dumps({'url': js['url'], 'relevance_score': js['relevance_score'], 'relevance_prediction': js['relevance_prediction']}) + '\\n')\n",
    "        \n",
    "cnt = 0\n",
    "for js in all_data:\n",
    "    if js['relevance_prediction'] and js['lang'] == 'en':\n",
    "        cnt += 1\n",
    "print(cnt)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
