{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socialsim as ss\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from glob import glob\n",
    "import itertools\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import OrderedDict, defaultdict, deque, Counter\n",
    "import pickle as pkl\n",
    "from scipy.stats.stats import pearsonr, spearmanr\n",
    "from scipy import stats, signal\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "import simplejson\n",
    "import gzip\n",
    "from math import sqrt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from joblib import parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles = defaultdict(dict)\n",
    "# with gzip.open('/data/leidos/2021CP5/CPEC/Exogenous/NewsArticles/cp5-cpec.exogenous.news-articles.v1.json.gz', 'r') as f:\n",
    "#     for l in f:\n",
    "#         tmp = json.loads(l)\n",
    "#         if 'extension' in tmp and 'earliest_reference_datetime' in tmp['extension'] and 'title' in tmp and 'text' in tmp:\n",
    "#             date = pd.to_datetime(tmp['extension']['earliest_reference_datetime']).date()\n",
    "#             if tmp['title'] not in articles[date]:\n",
    "#                 articles[date][tmp['title']] = [1, tmp['text']]\n",
    "#             else:\n",
    "#                 articles[date][tmp['title']][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "with open('/home/dmg/2021CP5/news_results.json', 'r') as f:\n",
    "    for l in f:\n",
    "        tmp = json.loads(l)\n",
    "        try:\n",
    "            if pd.to_datetime(tmp['date']) >= pd.to_datetime('2020-03-30') and pd.to_datetime(tmp['date']) <= pd.to_datetime('2020-08-31'):\n",
    "                json_data.append(tmp)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT TEMPLATE\n",
    "# with open('', 'r') as f:\n",
    "#     for l in f:\n",
    "#         tmp = json.loads(l)\n",
    "#         try:\n",
    "#             if pd.to_datetime(tmp['date']) >= pd.to_datetime('2020-03-30') and pd.to_datetime(tmp['date']) <= pd.to_datetime('2020-08-31'):\n",
    "#                 json_data.append(tmp)\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = defaultdict(lambda: defaultdict(dict))\n",
    "for r in json_data:\n",
    "#     if r['relevance_prediction'] and r['leidos_bert_prediction'] is not None and r['cleaned_article'] is not None and r['cleaned_article'] != '':\n",
    "    if r['leidos_bert_prediction'] is not None and r['cleaned_article'] is not None and r['cleaned_article'] != '':\n",
    "        for infoid in r['leidos_bert_prediction']:\n",
    "            if r['cleaned_title'] not in articles[infoid][pd.to_datetime(r['date']).date()]:\n",
    "                articles[infoid][pd.to_datetime(r['date']).date()][r['cleaned_title']] = [1, r['cleaned_article']]\n",
    "            else:\n",
    "                articles[infoid][pd.to_datetime(r['date']).date()][r['cleaned_title']][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = defaultdict(dict)\n",
    "for k, v in articles.items():\n",
    "    for kk, vv in tqdm(v.items(), total=len(v)):\n",
    "        tokens[k][kk] = word_tokenize(' '.join([' '.join([vvv[1]] * int(sqrt(vvv[0]))) for vvv in vv.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('\\n'.join([str(x) for x in articles[pd.to_datetime('2020-08-13').date()].keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + pd.read_csv('newstop.csv').term.to_list()) \n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctss = defaultdict(dict)\n",
    "for k, v in tokens.items():\n",
    "    for kk, vv in tqdm(v.items(), total=len(v)):\n",
    "        ctss[k][kk] = nltk.FreqDist([vvv.lower() for vvv in vv if len(vvv) > 3 and vvv.lower() not in stop_words and vvv.lower() in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for k1, v1 in ctss.items():\n",
    "#     print(k1)\n",
    "#     for i, (k, v) in enumerate(sorted(v1.items())):\n",
    "#         print(i)\n",
    "#         plt.figure(figsize=(18, 5))\n",
    "#         plt.title(k.strftime('%m/%d/%Y'))\n",
    "#         fig = v.plot(100).get_figure()\n",
    "#         fig.tight_layout()\n",
    "#         plt.show()\n",
    "#     fig.savefig('entropy/{0:03d}.png'.format(i))\n",
    "#     plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etps = defaultdict(dict)\n",
    "for k1, v1 in ctss.items():\n",
    "    for k, v in sorted(v1.items()):\n",
    "        tmp = v.most_common(100)\n",
    "        s = sum([x[1] for x in tmp])\n",
    "        ps = [x[1] / s for x in tmp]\n",
    "        H = -sum([x * np.log(x) for x in ps])\n",
    "        etps[k1][k] = -H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recifunc(x, s, c):\n",
    "    return (1 / np.power(x, s)) / np.sum([1 / np.power(i, s) for i in range(1, 101)]) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etps2 = defaultdict(dict)\n",
    "for k1, v1 in ctss.items():\n",
    "    for k, v in sorted(v1.items()):\n",
    "        try:\n",
    "            tmp = v.most_common(100)\n",
    "            s = sum([x[1] for x in tmp])\n",
    "            ps0 = [x[1] for x in tmp]\n",
    "            ps = [x[1] / s for x in tmp]\n",
    "            popt, pcov = curve_fit(recifunc, np.linspace(1, len(ps), len(ps)), ps)\n",
    "            etps2[k1][k] = popt[0]\n",
    "        except:\n",
    "            print(k1, k, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append = 'to_7_27'\n",
    "with open('/data/leidos_extracted/2021CP5/twitter_time_series_{}.json'.format(append), 'r') as f:\n",
    "    d = json.loads(f.read())\n",
    "tdf = {k: pd.read_json(v, orient='columns').EventCount for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/leidos_extracted/2021CP5/cp5_eval_nodes.txt', 'r') as f:\n",
    "    nodes = f.read().split('\\n')[:-2]\n",
    "with open('/data/leidos_extracted/2021CP5/cp5_other_nodes.txt', 'r') as f:\n",
    "    nodes += f.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/leidos_extracted/2021CP5/gdelt_time_series.json', 'r') as f:\n",
    "    gdelt_ts = json.load(f)\n",
    "gdelt = OrderedDict(sorted({k: pd.read_json(v, typ='series') for k, v in gdelt_ts.items()}.items(), key=lambda kv: kv[1].sum(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/dmg/2021CP5/news_west_300_time_series_to_6_29.json', 'r') as f:\n",
    "    west_ts = json.load(f)\n",
    "west = OrderedDict(sorted({k: pd.read_json(v, typ='series') for k, v in west_ts.items()}.items(), key=lambda kv: kv[1].sum(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/michael/dryrun/corrmap.json', 'r') as f:\n",
    "    twitter_corr_data = json.load(f)['twitter']\n",
    "twitter_corr_data = OrderedDict(\n",
    "        sorted({\n",
    "            k: OrderedDict(\n",
    "                sorted(\n",
    "                    v.items(), \n",
    "                    key=lambda kv:(kv[1], kv[0]), \n",
    "                    reverse=True\n",
    "                )\n",
    "            ) for k, v in twitter_corr_data.items()\n",
    "        }.items())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/data/leidos_extracted/2021CP5/cp5_eval_nodes.txt', 'r') as f:\n",
    "#     nodes = f.read().split('\\n')[:-2]\n",
    "idx = pd.date_range(pd.to_datetime('2020-03-30'), pd.to_datetime('2020-08-30'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in etps2.items():\n",
    "    if len(v) == 0:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etps2_df = {}\n",
    "for k, v in etps2.items():\n",
    "    tmpmean = np.mean(list(v.values()))\n",
    "    tmp = pd.Series(v).reindex(idx, fill_value=tmpmean)\n",
    "    tmp[tmp < 1e-3] = tmpmean\n",
    "    etps2_df[k] =tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in set(nodes) - set(etps2_df.keys()):\n",
    "    etps2_df[k] = pd.Series().reindex(idx, fill_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etps2_df_json = {k: v.to_json() for k, v in etps2_df.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/leidos_extracted/2021CP5/zipf_time_series_to_8_31.json', 'w') as f:\n",
    "    f.write(json.dumps(etps2_df_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('ssenv': conda)",
   "language": "python",
   "name": "python36764bitssenvconda064968db67b24473b1fc8e22de9d498f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
